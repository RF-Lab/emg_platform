## Входные данные
В качестве входных данных использовались 79 векторов длиной 481 единицы для каждого жеста, что предоставляет нам окно, в котором зафиксирован потенциал действия.

Набор жестов включают следующие движения:
- кисть вверх (класс 0);
- кисть вниз (класс 1);
- сжатие всех пальцев (класс 2);
- сжатие указ пальца (класс 3);
- сжатие среднего пальца (класс 4);
- сжатие безымянного пальца (класс 5);
- щелчок большого с средним (класс 6);
- разжимание всех пальцев (класс 7);
- поворот кисти влево (класс 8);
- поворот кисти вправо (класс 9)

Т.е. общее количество исследуемых ЭМГ-сигналов - 790 (`emg_platform\data\data10mov_no_abs.mat`)

## Алгоритмическая реализация
Основная часть работы программного решения может быть представлена в виде структурно-функциональной схемы на рисунке 1:

<p align="center">
    <img alt="СтруктурноФункциональнаяСхемаПО" src="https://user-images.githubusercontent.com/31689842/59972945-a78f4080-95a0-11e9-84d2-33569bfb515c.png"/>
</p>

_**Рисунок 1**. Структурно-функциональная схема основной части работы программы._

Вся программная реализация основана на ядерном методе опорных векторов в режиме один против одного. В качестве вектора признаков сигнала выступал сам вектор сигнала ЭМГ. Все сигналы делятся на тренировочные и тестовые данные в соотношении 2 к 1. Диапазон значений параметров из сетки параметров выставлен экспериментальным путем. Для каждого рассмотренного в исследовании ядра существует своя сетка параметров для увеличения производительности.

## Предобработка и нормализация данных

> SVM очень чувствителен к масштабированию данных. Поэтому обычной практикой является преобразование признаков с тем, чтобы итоговое представление данных было более подходящим. Часто достаточно простого нормализации признаков и корректировки данных.

В рамках данного исследования было проанализировано 8 методов предобработки и нормализации данных:
- стандартное масштабирование (Standard Scaler)

<p align="center">
    <img width="190" alt="Standard Scaler" src="https://user-images.githubusercontent.com/31689842/59973288-9b0de680-95a6-11e9-8914-2a8983be0cc8.png"/>
</p>

- min-max масштабирование (Min-Max Scaler)

<p align="center">
    <img width="180" alt="Min-Max Scaler" src="https://user-images.githubusercontent.com/31689842/59973306-df00eb80-95a6-11e9-805a-b98a85680383.png"/>
</p>

- масштабирование по максимальному абсолютному значению (Max-Abs Scaler)

<p align="center">
    <img align="middle" width="280" alt="Max-Abs Scaler" src="https://user-images.githubusercontent.com/31689842/59973368-ce9d4080-95a7-11e9-8165-296964448195.png"/>
</p>

- устойчивое масштабирование (Robust Scaler)

<p align="center">
    <img align="middle" width="200" alt="Robust Scaler" src="https://user-images.githubusercontent.com/31689842/59973374-e5dc2e00-95a7-11e9-887b-95e0c9fbad29.png"/>
</p>

- L2 нормализация (просто Normalizer)

<p align="center">
    <img align="middle" width="160" alt="Normalizer" src="https://user-images.githubusercontent.com/31689842/59977529-ee019100-95da-11e9-99e5-9f6ca59328f1.png"/>
</p>

- степенное преобразование Йео-Джонсона (Yeo-Johnson Power Transformer)

<p align="center">
    <img align="middle" width="390" alt="Yeo-Johnson Power Transformer" src="https://user-images.githubusercontent.com/31689842/59973397-250a7f00-95a8-11e9-9935-48d0a9a69cec.png"/>
</p>

- квантильное преобразование c нормальным распределением (Normal Quantile Transformer)
- квантильное преобразование с равномерным распределением (Uniform Quantile Transformer)

<p align="center">
    <img width="590" alt="Quantile Transformer" src="https://user-images.githubusercontent.com/31689842/59973404-3eabc680-95a8-11e9-8a9b-b3f2b46702a9.png"/>
</p><p align="center">
    <img width="290" alt="Uniform Quantile Transformer" src="https://user-images.githubusercontent.com/31689842/59973412-5aaf6800-95a8-11e9-9626-fabe0c735dc4.png"/>
</p><p align="center">
    <img width="330" alt="Normal Quantile Transformer" src="https://user-images.githubusercontent.com/31689842/59973417-6b5fde00-95a8-11e9-963a-891b00820aee.png"/>
</p>

Оценка качества вышеупомянутых методов предобработки и нормализации для 5 и 10 классов представлена в таблице 1 и 2. Параметры регуляризации SVM и его ядра найдены случайным решетчатым поиском с 5-блочной кросс-валидацией.

_**Таблица 1**. Точность классификации при различных методах предобработки и нормализации для 5 и 10 классов и RBF ядра._

<p align="center">
    <img width="700" alt="Таблица 1" src="https://user-images.githubusercontent.com/31689842/59973886-d7911080-95ad-11e9-84f4-e5678797d676.png"/>
</p>

_**Таблица 2**. Точность классификации при различных методах предобработки и нормализации для 5 и 10 классов и полиномиального ядра._

<p align="center">
    <img width="700" alt="Таблица 2" src="https://user-images.githubusercontent.com/31689842/59977567-46d12980-95db-11e9-9c2f-683407a985ce.png"/>
</p>

## Ядра SVM-классификатора

> Ядра – с одной стороны, одно из самых красивых и плодотворных изобретений в машинном обучении, с другой стороны, до сих пор не найдено эффективного общего подхода к их подбору в конкретных задачах. То есть с математической точки зрения ядром может служить любая положительно определённая симметричная функция двух переменных. Положительная определённость необходима для того, чтобы соответствующая функция Лагранжа в задаче оптимизации была ограничена снизу, т.е. задача оптимизации была бы корректно определена.

В рамках данного исследования работы были проанализированы ядра, которые чаще всего встречаются на практике:
- линейное ядро (linear kernel)

<p align="center">
    <img height="32" alt="linear kernel" src="https://user-images.githubusercontent.com/31689842/59974019-b0d3d980-95af-11e9-8857-7850d6b822b3.png"/>
</p>

- радиальная базисная функция (RBF kernel)

<p align="center">
    <img height="45" alt="RBF kernel" src="https://user-images.githubusercontent.com/31689842/59974051-1fb13280-95b0-11e9-8134-204305ac3ff4.png"/>
</p>

- сигмоидальное ядро (sigmoid kernel)

<p align="center">
    <img height="32" alt="sigmoid kernel" src="https://user-images.githubusercontent.com/31689842/59974080-5f781a00-95b0-11e9-81a0-57a314bf1b78.png"/>
</p>

- полиномиальное ядро (polynomial kernel)

<p align="center">
    <img height="37" alt="polynomial kernel" src="https://user-images.githubusercontent.com/31689842/59974092-833b6000-95b0-11e9-8192-d48acd1b00d5.png"/>
</p>

Результаты анализа отображены на графиках эффективности ядер по тестовым и тренировочным данным для 10 классов (рисунок 2) и для 5 классов (рисунок 3) с применением различных методов предобработки и нормализации данных.

<p align="center">
    <img width="550" alt="Standard Scaler and Min-Max Scaler" src="https://user-images.githubusercontent.com/31689842/59974177-a7e40780-95b1-11e9-98f9-c58057cf6d66.png"/>
    <img width="550" alt="Max-Abs Scaler and Normalizer" src="https://user-images.githubusercontent.com/31689842/59974181-add9e880-95b1-11e9-9da5-4386aaea920c.png"/>
    <img width="550" alt="Robust Scaler and Yeo-Johnson Power Transformer" src="https://user-images.githubusercontent.com/31689842/59974183-b16d6f80-95b1-11e9-8f50-3eb3da59efea.png"/>
    <img width="550" alt="Uniform Quantile Transformer and Normal Quantile Transformer" src="https://user-images.githubusercontent.com/31689842/59974184-b29e9c80-95b1-11e9-9433-c7ae462a1338.png"/>
</p>

_**Рисунок 2**. Графики эффективности ядер для 10 классов с применением различных методов предобработки и нормализации данных._

<p align="center">
    <img width="550" alt="Standard Scaler and Min-Max Scaler" src="https://user-images.githubusercontent.com/31689842/59974228-5e47ec80-95b2-11e9-98ac-7ce0d20ec1cf.png"/>
    <img width="550" alt="Max-Abs Scaler and Normalizer" src="https://user-images.githubusercontent.com/31689842/59974229-5f791980-95b2-11e9-9031-824ea2c5d54f.png"/>
    <img width="550" alt="Robust Scaler and Yeo-Johnson Power Transformer" src="https://user-images.githubusercontent.com/31689842/59974230-60aa4680-95b2-11e9-8f36-620b2542ff29.png"/>
    <img width="550" alt="Uniform Quantile Transformer and Normal Quantile Transformer" src="https://user-images.githubusercontent.com/31689842/59974231-61db7380-95b2-11e9-8f2f-cf3606ed1f95.png"/>
</p>

_**Рисунок 3**. Графики эффективности ядер для 5 классов с применением различных методов предобработки и нормализации данных._

## Гиперпараметры SVM-классификатора

Поиск оптимальных значений ключевых параметров модели (то есть значений, которые дают наилучшую обобщающую способность) является сложной задачей, но она обязательна почти для всех моделей и наборов данных. В рамках данного исследования было рассмотрено два метода: решетчатый поиск (grid search), который по сути является попыткой перебрать все возможные комбинации интересующих нас параметров и случайный поиск ([Bergstra J., Bengio Y. Random search for hyper-parameter optimization – 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)), призванный ограничить экспоненциальный рост вычислительных трудозатрат у решетчатого поиска.

Смысл решетчатого поиска (`sklearn.model_selection.GridSearchCV`) заключается в том, что множество параметров регуляризации ядерного SVM формирует сетку параметров, для каждого элемента которого выполняется k-блочная кросс-валидация для избежания переобучения. Так, например, при радиальной базисной функции, если `C = [1, 2, … , 5]`, `γ = [0.01, 0.02, … , 0.1]` и используется 5-блочная перекрёстная проверка, то для поиска наилучших параметров, которые дадут наиболее высокую точность классификации на тестовых данных, понадобится `5 * 10 * 5 = 250` раз обучить модель.

У обычного решетчатого поиска очевидно, что есть большой недостаток, время его работы растет очень бычтро при увеличении числа параметров и возможных их значений. Для решения данной проблемы используется случайны поиск (`sklearn.model_selection.RandomizedSearchCV`), который предполагает ограничение отбираемых элементов из решетки. Отбор осуществляется случайным образом без дублирования.

Было проведено несколько испытаний обоих подходов: результаты отражены в таблице 3. В испытаниях использовалось: 5-блочная перекрёстная проверка; случайный поиск, ограниченный 10 итерациями; 4 ранее рассмотренных ядра SVM; 8 методов предобработки и нормализации, которые также были ранее рассмотрены. Результаты для различных методов предобработки и нормализации в таблицу 3 внесены небыли, были добавлены только те, которые показали лучшую точность классификации на тестовых данных.

_**Таблица 3**. Результаты различных методов поиска гиперпараметров для 5 классов._

<p align="center">
    <img height="420" alt="Таблица 3" src="https://user-images.githubusercontent.com/31689842/59974473-3efe8e80-95b5-11e9-8f4e-edaa8ce6ea50.png"/>
</p>

## Выводы

В результате исследования были сделаны следующие выводы:
1.	Несмотря на небольшое ухудшение в точности классификации случайный решетчатый поиск с перекрестной проверкой демонстрирует боле высокую степень эффективности решения задачи подбора наилучших гипермараметров SVM в сравнении с обычным решетчатым поиском. При увеличении диапазона значений параметров регуляризации SVM-классификатора, точность может начать резко снижаться, поэтому для решения этой проблемы, необходимо увеличить количество отбираемых элементов из сетки.
2.	Из рассматриваемых методов предобработки и нормализации данных нельзя выделить какой-то один наилучший алгоритм, так как максимальная точность классификации на тренировочных данных достигается различными методами для различных наборов классов. Поэтому необходимо сначала для конкретного набора оценить среднюю точность классификации всех подходов предобработки и нормализации, а потом уже формировать окончательную модель.
3.	Наиболее результативными ядрами SVM-классификатора из рассмотренных являются полиномиальное ядро и RBF. Хоть линейное ядро и выдавала иногда хороший результат занимая второе место по точности классификации на тестовых данных, использовать его не целесообразно, так как полиномиальное ядро и RBF взаимно дополняют друг друга: если один оказался нерезультативным, то другой с большой вероятностью покажет наиболее высокую точность классификации. Сигмоидальное же ядро оказался самым худшим ядром для исследуемого набора данных и не рекомендуется для использования его в ядерном методе опорных векторов в режиме один против одного при классификации сигналов электромиографии.

_**Автор**: Семендаров А.В. (asemendarov@lavabit.com)_